# MultiLayer Perceptron

This project is an implementation of a MultiLayer Perceptron (MLP) from scratch using Python and NumPy. It demonstrates fundamental concepts of neural networks, such as feedforward computation, backpropagation, and gradient descent.

## About

The purpose of this project is to:
- Understand and implement the mathematical foundations of neural networks.
- Gain hands-on experience in building machine learning models without relying on external libraries like TensorFlow or PyTorch.
- Experiment with different activation functions and training techniques.

## Highlights

- Custom-built MLP architecture.
- Implements activation functions such as sigmoid, tanh, and ReLU.
- Includes backpropagation and gradient descent algorithms.
- Interactive UI for observing the model's perfomance real-time.

## Motivation

This project was created as a learning exercise to deepen my understanding of the inner workings of neural networks and machine learning principles. It is not intended for production use.

## Notes

The repository is primarily for educational purposes and to document my progress. Feel free to explore the code, but keep in mind that it is not optimized for general usage.

## License

This project is shared under an MIT License. See the [LICENSE](LICENSE) file for details.
